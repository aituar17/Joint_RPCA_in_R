options(warn = -1)
#load user-defined functions
source("../R/dependencies.R")
source("../R/jointRPCA.R")
source("../R/jointRPCAmae.R")
source("../R/jointOptspaceHelper.R")
source("../R/jointOptspaceSolve.R")
source("../R/optspaceHelper.R")
source("../R/transformHelper.R")
source("../R/transform.R")
source("../R/maskValueOnly.R")
source("../R/rpcaTableProcessing.R")
source("../R/jointRPCAutils.R")

suppressPackageStartupMessages({
library(mia)
library(ggplot2)
})

#extract sample and feature scores
samples_df <- as.data.frame(result$ord.res$samples)
samples_df$Label <- rownames(samples_df)

#get test/train labels from metadata
sample_metadata <- as.data.frame(SummarizedExperiment::colData(HintikkaXOData))
sample_metadata$Label <- rownames(sample_metadata)

#manually tag samples as train/test
sample_metadata$Set <- c(rep("train", 7), rep("test", nrow(sample_metadata) - 7))

#merge with ordination scores
samples_df <- merge(samples_df, sample_metadata[, c("Label", "Set")],
                    by = "Label", all.x = TRUE)

#feature scores
features_df <- as.data.frame(result$ord.res$features)
features_df$Label <- rownames(features_df)

#sample ordination plot
ggplot(samples_df, aes(x = PC1, y = PC2, color = Set)) +
  geom_point(size = 3) +
  geom_text(aes(label = Label), vjust = -1.2) +
  theme_minimal() +
  scale_color_manual(values = c("train" = "steelblue", "test" = "tomato")) +
  labs(title = "Joint RPCA Ordination on HintikkaXOData", x = "PC1", y = "PC2")

# Feature Importance Analysis

#rank features by contribution
loadings <- result$ord.res$features

ranked_features <- lapply(colnames(loadings), function(pc) {
  df <- data.frame(
    Feature = rownames(loadings),
    Loading = loadings[, pc],
    AbsLoading = abs(loadings[, pc])
  )
  df <- df[order(-df$AbsLoading), ]
  rownames(df) <- NULL
  df
})
names(ranked_features) <- colnames(loadings)

#view top features
head(ranked_features$PC1, 5)
head(ranked_features$PC2, 5)

# Visualize Top Features Driving PC1

top_PC1 <- head(ranked_features$PC1, 10)

ggplot(top_PC1, aes(x = reorder(Feature, AbsLoading), y = AbsLoading)) +
  geom_col(fill = "darkslateblue") +
  coord_flip() +
  labs(title = "Top Features Driving PC1 in HintikkaXOData",
       x = NULL, y = "Absolute Loading")


# Compute and Visualize Covariance Matrix of Feature Loadings

# Covariance matrix of joint feature loadings

# Extract joint feature loadings
feature_loadings <- metadata(HintikkaXOData[["microbiota"]])$jointRPCA_feature_loadings

# Subset the numeric columns (PC axes)
loadings_matrix <- as.matrix(feature_loadings[, c("PC1", "PC2")])

# Compute a feature-feature covariance matrix
pairwise_cov <- tcrossprod(loadings_matrix)

# Melt the covariance matrix to long format
cov_long <- melt(pairwise_cov)
names(cov_long) <- c("Feature1", "Feature2", "Covariance")

# Plot covariance heatmap
ggplot(cov_long, aes(x = Feature1, y = Feature2, fill = Covariance)) +
  geom_tile() +
  scale_fill_gradient2(low = "darkred", high = "darkgreen", mid = "white",
                       midpoint = 0, limit = c(min(cov_long$Covariance), max(cov_long$Covariance)),
                       name = "Covariance") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Covariance Matrix of RPCA Feature Loadings",
       x = "Feature", y = "Feature")



mofa_object <- create_mofa(mofa_data)
data_opts <- get_default_data_options(mofa_object)
model_opts <- get_default_model_options(mofa_object)
train_opts <- get_default_training_options(mofa_object)
train_opts$maxiter <- 1000
mofa_prep <- prepare_mofa(
  object = mofa_object,
  data_options = data_opts,
  model_options = model_opts,
  training_options = train_opts
)
mofa_trained <- run_mofa(mofa_prep)
factors_list <- get_factors(mofa_trained, factors = "all", as.data.frame = FALSE)
features_mofa <- factors_list[[1]]
labels_mofa <- labels[match(rownames(features_mofa), names(labels))]

#prepare randomly generated features as an input feature set
set.seed(42)
features_random <- matrix(runif(length(labels) * 10), ncol = 10)

#create a classification function (Random Forest)
evaluate_model_cv <- function(features, labels, folds = 5) {
  labels <- as.factor(labels)
  folds_idx <- createFolds(labels, k = folds, list = TRUE, returnTrain = FALSE)
  
  accs <- c()
  aucs <- c()
  
  for (i in seq_along(folds_idx)) {
    test_idx <- folds_idx[[i]]
    train_idx <- setdiff(seq_along(labels), test_idx)
    
    rf_model <- randomForest(x = features[train_idx, ], y = labels[train_idx], ntree = 500)
    pred_class <- predict(rf_model, features[test_idx, ])
    acc <- mean(pred_class == labels[test_idx])
    
    pred_probs <- predict(rf_model, features[test_idx, ], type = "prob")
    auc <- multiclass.roc(labels[test_idx], pred_probs)$auc
    
    accs <- c(accs, acc)
    aucs <- c(aucs, auc)
  }
  
  return(list(accuracy = mean(accs), auc = mean(aucs)))
}

#evaluate all methods
res_joint <- evaluate_model_cv(features_jointRPCA, labels)
res_rclr <- evaluate_model_cv(features_rclr_concat, labels)
res_pca <- evaluate_model_cv(features_pca_concat, labels)
res_rpca <- evaluate_model_cv(features_rpca_concat, labels)
res_mofa <- evaluate_model_cv(features_mofa, labels_mofa)
res_random <- evaluate_model_cv(features_random, labels)

#create and print a summary table
results_df <- data.frame(
  Method = c("Joint-RPCA", "Raw rCLR-Transformed Features", "Per-layer PCA", "Per-layer RPCA", "MOFA+", "Random"),
  Accuracy = c(res_joint$accuracy, res_rclr$accuracy, res_pca$accuracy, res_rpca$accuracy, res_mofa$accuracy, res_random$accuracy),
  AUC = c(res_joint$auc, res_rclr$auc, res_pca$auc, res_rpca$auc, res_mofa$auc, res_random$auc)
)

print(results_df)






#create label vector
labels <- colData(HintikkaXOData)$Diet

#prepare different input feature sets

#prepare Joint-RPCA sample scores as an input feature set
features_jointRPCA <- result$ord.res$samples

#prepare RPCA sample scores per omic, then concatenated, as an input feature set
features_rpca_concat <- do.call(cbind, dataset_specific_scores)

#prepare single-layer RPCA sample scores (layer 1) as an input feature set
features_layer1 <- dataset_specific_scores[[1]]

#prepare randomly generated features as an input feature set
set.seed(42)
features_random <- matrix(runif(length(labels) * 10), ncol = 10)

#create a classification function (Random Forest)
evaluate_model_cv <- function(features, labels, folds = 5) {
  labels <- as.factor(labels)
  folds_idx <- createFolds(labels, k = folds, list = TRUE, returnTrain = FALSE)
  
  accs <- c()
  aucs <- c()
  
  for (i in seq_along(folds_idx)) {
    test_idx <- folds_idx[[i]]
    train_idx <- setdiff(seq_along(labels), test_idx)
    
    rf_model <- randomForest(x = features[train_idx, ], y = labels[train_idx], ntree = 500)
    pred_class <- predict(rf_model, features[test_idx, ])
    acc <- mean(pred_class == labels[test_idx])
    
    pred_probs <- predict(rf_model, features[test_idx, ], type = "prob")
    auc <- multiclass.roc(labels[test_idx], pred_probs)$auc
    
    accs <- c(accs, acc)
    aucs <- c(aucs, auc)
  }
  
  return(list(accuracy = mean(accs), auc = mean(aucs)))
}

#evaluate all methods
res_joint <- evaluate_model_cv(features_jointRPCA, labels)
res_rpca <- evaluate_model_cv(features_rpca_concat, labels)
res_layer1 <- evaluate_model_cv(features_layer1, labels)
res_random <- evaluate_model_cv(features_random, labels)

#create and print a summary table
results_df <- data.frame(
  Method = c("Joint-RPCA", "Per-layer RPCA", "Single-layer RPCA", "Random"),
  Accuracy = c(res_joint$accuracy, res_rpca$accuracy, res_layer1$accuracy, res_random$accuracy),
  AUC = c(res_joint$auc, res_rpca$auc, res_layer1$auc, res_random$auc)
)

print(results_df)











# 1) Choose/derive labels (factor) --------------------------------------------
labels <- as.factor(colData(HintikkaXOData)$Diet)

# 2) Build a common sample index across all feature sets ----------------------
#    (joint scores and rclr tables share rownames)
rn_joint <- rownames(result$ord.res$samples)
rn_rclr  <- lapply(result$rclr.tables, rownames)
common_samples <- Reduce(intersect, lapply(result$rclr.tables, colnames))

# Helper: drop columns with 0 variance or all-NA/constant
drop_constant_cols <- function(X) {
  sds <- apply(X, 2, function(v) sd(v, na.rm = TRUE))
  keep <- is.finite(sds) & (sds > 0)
  if (!any(keep)) stop("No non-constant columns remain after filtering.")
  X[, keep, drop = FALSE]
}

# Drop columns that contain any NA/NaN/Inf
keep_finite_cols <- function(X) {
  ok <- apply(X, 2, function(v) all(is.finite(v)))
  if (!any(ok)) stop("All columns removed by finite filter.")
  X[, ok, drop = FALSE]
}

# Drop columns with zero variance (after finite filter)
drop_constant_cols <- function(X) {
  sds <- apply(X, 2, function(v) sd(v, na.rm = TRUE))
  keep <- is.finite(sds) & (sds > 0)
  if (!any(keep)) stop("No non-constant columns remain after filtering.")
  X[, keep, drop = FALSE]
}

# ---- Concatenated rCLR -> PCA (global PCA baseline) ----
X_list <- lapply(result$rclr.tables, function(tbl) t(tbl[, common_samples, drop = FALSE]))  # samples × features
features_rclr_concat <- do.call(cbind, X_list)

# 1) finite filter, 2) constant filter, THEN PCA
features_rclr_concat <- keep_finite_cols(features_rclr_concat)
features_rclr_concat <- drop_constant_cols(features_rclr_concat)

pca_concat <- prcomp(features_rclr_concat, center = TRUE, scale. = TRUE)
features_concat_pca <- pca_concat$x[, 1:10, drop = FALSE]

# ---- Per-layer PCA -> concatenate ----
K_pcs <- 3
dataset_specific_pca_scores <- lapply(result$rclr.tables, function(tbl) {
  X <- t(tbl[, common_samples, drop = FALSE])            # samples × features
  X <- keep_finite_cols(X)
  X <- drop_constant_cols(X)
  pca <- prcomp(X, center = TRUE, scale. = TRUE)
  k <- min(K_pcs, ncol(pca$x))
  pca$x[, seq_len(k), drop = FALSE]
})
features_pca_concat <- do.call(cbind, dataset_specific_pca_scores)

# ---- Per-layer RPCA -> concatenate (aligned) ----
dataset_specific_scores_aligned <- lapply(dataset_specific_scores, function(S) {
  S <- S[common_samples, , drop = FALSE]
  S <- keep_finite_cols(S)          # safe-guard
  drop_constant_cols(S)             # in case a component is constant on common samples
})
features_rpca_concat <- do.call(cbind, dataset_specific_scores_aligned)

# ---- Joint-RPCA shared scores (aligned) ----
features_jointRPCA <- result$ord.res$samples[common_samples, , drop = FALSE]
features_jointRPCA <- keep_finite_cols(features_jointRPCA)
features_jointRPCA <- drop_constant_cols(features_jointRPCA)

# ---- Random baseline ----
set.seed(1)
features_random <- matrix(rnorm(length(common_samples) * 10), nrow = length(common_samples), ncol = 10)
rownames(features_random) <- common_samples
colnames(features_random) <- paste0("rand_", seq_len(ncol(features_random)))

# ---- Align labels to the same sample order ----
labels <- droplevels(as.factor(colData(HintikkaXOData)$Diet)[common_samples])

stopifnot(
  nrow(features_jointRPCA) == length(labels),
  nrow(features_rpca_concat) == length(labels),
  nrow(features_pca_concat)  == length(labels),
  nrow(features_rclr_concat) == length(labels),
  nrow(features_random)      == length(labels)
)

# ========== Cross-validated evaluation helpers ==========
suppressPackageStartupMessages({
  library(caret)
  library(pROC)
  library(randomForest)
  library(dplyr)
  library(tidyr)
  library(ggplot2)
})

# Preprocess (center/scale) WITHOUT leakage: fit on train only, apply to test
prep_train_test <- function(X_train, X_test) {
  m <- colMeans(X_train, na.rm = TRUE)
  s <- apply(X_train, 2, sd, na.rm = TRUE)
  s[s == 0 | !is.finite(s)] <- 1
  list(
    Xtr = sweep(sweep(X_train, 2, m, "-"), 2, s, "/"),
    Xte = sweep(sweep(X_test,  2, m, "-"), 2, s, "/")
  )
}

# ---- Align labels to the same sample order ----
labels <- as.factor(colData(HintikkaXOData)$Diet)[common_samples]

# ---- Align labels to the same sample order ----
labels0 <- as.factor(colData(HintikkaXOData)$Diet)[common_samples]

# 1) drop NAs
keep_idx <- !is.na(labels0)

# 2) drop classes with < 2 samples
tab0 <- table(labels0[keep_idx])
keep_classes <- names(tab0)[tab0 >= 2]
keep_idx <- keep_idx & labels0 %in% keep_classes

# 3) sanity checks
if (!any(keep_idx)) stop("After filtering NAs and rare classes, no samples remain.")
labels <- droplevels(labels0[keep_idx])
if (nlevels(labels) < 2) stop("Need at least 2 classes after filtering.")
tab <- table(labels)

# 4) subset features to the safe set
features_jointRPCA <- features_jointRPCA[keep_idx, , drop = FALSE]
features_rpca_concat <- features_rpca_concat[keep_idx, , drop = FALSE]
features_pca_concat  <- features_pca_concat[keep_idx, , drop = FALSE]
features_concat_pca  <- features_concat_pca[keep_idx, , drop = FALSE]
features_random      <- features_random[keep_idx, , drop = FALSE]

# 5) choose a safe K for CV
safe_k <- max(2L, min(5L, as.integer(min(tab)), length(labels) - 1L))

evaluate_model_cv <- function(features, labels, folds = 5, ntree = 500, seed = 42) {
  set.seed(seed)

  # recompute safe k with current labels
  tab <- table(labels)
  if (length(labels) < 2L || length(tab) < 2L) stop("Need >=2 samples and >=2 classes.")
  folds <- max(2L, min(as.integer(folds), as.integer(min(tab)), length(labels) - 1L))

  folds_idx <- caret::createFolds(labels, k = folds, list = TRUE, returnTrain = FALSE)

  accs <- numeric(length(folds_idx))
  aucs <- numeric(length(folds_idx))

  for (i in seq_along(folds_idx)) {
    test_idx  <- folds_idx[[i]]
    train_idx <- setdiff(seq_along(labels), test_idx)

    Xtr <- features[train_idx, , drop = FALSE]
    Xte <- features[test_idx,  , drop = FALSE]
    ytr <- labels[train_idx]
    yte <- labels[test_idx]

    # skip degenerate train folds
    if (length(unique(ytr)) < 2L) { accs[i] <- NA_real_; aucs[i] <- NA_real_; next }

    pp <- prep_train_test(Xtr, Xte)
    rf <- randomForest::randomForest(x = pp$Xtr, y = ytr, ntree = ntree)

    yhat <- predict(rf, pp$Xte, type = "response")
    accs[i] <- mean(yhat == yte)

    probs <- predict(rf, pp$Xte, type = "prob")
    # ensure all class columns exist
    all_lvls <- levels(labels)
    miss <- setdiff(all_lvls, colnames(probs))
    if (length(miss)) for (mm in miss) probs <- cbind(probs, setNames(rep(0, nrow(probs)), mm))
    probs <- probs[, all_lvls, drop = FALSE]

    if (length(unique(yte)) < 2L) {
      aucs[i] <- NA_real_
    } else {
      aucs[i] <- tryCatch(as.numeric(pROC::multiclass.roc(yte, probs)$auc), error = function(e) NA_real_)
    }
  }

  list(accuracy = mean(accs, na.rm = TRUE), auc = mean(aucs, na.rm = TRUE))
}

# ========== Run all baselines ==========
res_joint   <- evaluate_model_cv(features_jointRPCA, labels, folds = safe_k)
res_rpca    <- evaluate_model_cv(features_rpca_concat, labels, folds = safe_k)
res_pca     <- evaluate_model_cv(features_pca_concat,  labels, folds = safe_k)
res_concat  <- evaluate_model_cv(features_concat_pca,  labels, folds = safe_k)
res_random  <- evaluate_model_cv(features_random,      labels, folds = safe_k)

results_df <- tibble::tibble(
  Method   = c("Joint‑RPCA (shared scores)",
               "Per‑layer RPCA → concat",
               "Per‑layer PCA → concat",
               "Concatenated rCLR → PCA",
               "Random"),
  Accuracy = c(res_joint$accuracy,
               res_rpca$accuracy,
               res_pca$accuracy,
               res_concat$accuracy,
               res_random$accuracy),
  MacroAUC = c(res_joint$auc,
               res_rpca$auc,
               res_pca$auc,
               res_concat$auc,
               res_random$auc)
) %>%
  arrange(desc(MacroAUC))

print(results_df)

# Quick visualization
results_long <- results_df |>
  tidyr::pivot_longer(cols = c(Accuracy, MacroAUC), names_to = "Metric", values_to = "Score")

ggplot(results_long, aes(x = reorder(Method, Score), y = Score)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ Metric, scales = "free_x") +
  labs(x = NULL, y = "Score", title = "Classification benchmarks (5‑fold CV)") +
  theme_minimal(base_size = 12)







# 1) Choose/derive labels (factor) --------------------------------------------
labels <- as.factor(colData(HintikkaXOData)$Diet)

# 2) Build a common sample index across all feature sets ----------------------
#    (joint scores and rclr tables share rownames)
rn_joint <- rownames(result$ord.res$samples)
rn_rclr  <- lapply(result$rclr.tables, rownames)
common_samples <- Reduce(intersect, lapply(result$rclr.tables, colnames))

# --- Find a viable label column automatically among common_samples ---
cd <- as.data.frame(colData(HintikkaXOData)[common_samples, , drop = FALSE])

is_categorical <- function(x) is.factor(x) || is.character(x)
cats <- names(cd)[vapply(cd, is_categorical, logical(1))]

viable <- list()
for (nm in cats) {
  x <- as.factor(cd[[nm]])
  x <- droplevels(x)
  x <- x[!is.na(x)]
  if (nlevels(x) >= 2) {
    ct <- table(x)
    if (min(ct) >= 2) viable[[nm]] <- ct
  }
}

if (length(viable) == 0L) {
  stop("No categorical column in colData has ≥2 classes with ≥2 samples each among common_samples.")
}

# pick the "best" (most balanced) label by largest min class size, then largest total
score <- vapply(viable, function(ct) c(min = min(ct), total = sum(ct)), numeric(2))
best_idx <- order(score["min", ], score["total", ], decreasing = TRUE)[1]
target_label <- names(viable)[best_idx]
message("Using label column: ", target_label)

labels0 <- as.factor(cd[[target_label]])

print(target_label)

print(labels0)








---
title: "Joint RPCA Reproducible Example"
format: html
editor: visual
---

```{r setup, message=FALSE, warning=FALSE}
options(warn = -1)
# Load user-defined functions
source("../R/dependencies.R")
source("../R/jointRPCA.R")
source("../R/jointRPCAuniversal.R")
source("../R/jointOptspaceHelper.R")
source("../R/jointOptspaceSolve.R")
source("../R/optspaceHelper.R")
source("../R/transformHelper.R")
source("../R/transform.R")
source("../R/maskValueOnly.R")
source("../R/rpcaTableProcessing.R")
source("../R/jointRPCAutils.R")

# Load Data and Run Joint RPCA

# Example using MAE from mia::HintikkaXOData

#load example data
data(HintikkaXOData)

#run joint RPCA on MultiAssayExperiment object
result <- jointRPCAuniversal(
  data = HintikkaXOData,
  n.components = 3,
  train.test.column = "Set",
  rclr.transform.tables = TRUE,
  max.iterations = 5
)

# Compute and Store Sample Scores

#compute dataset-specific sample scores
rclr.tables <- result$rclr.tables
dataset_specific_scores <- .dataset_specific_scores(rclr.tables, n.components = 3, max.iterations = 5)

#store dataset-specific sample scores per experiment
for (i in seq_along(dataset_specific_scores)) {
  experiment_name <- names(HintikkaXOData)[i]
  reducedDim(HintikkaXOData[[experiment_name]], "localRPCA") <- dataset_specific_scores[[i]]
}

#view dataset-specific sample scores
for (name in names(HintikkaXOData)) {
  cat("\nSample Scores for:", name, "\n")
  print(head(reducedDim(HintikkaXOData[[name]], "localRPCA")))
}

#store joint RPCA sample scores in taxonomic experiment
reducedDim(HintikkaXOData[["microbiota"]], "jointRPCA") <- result$ord.res$samples

#view joint RPCA sample scores
head(reducedDim(HintikkaXOData[["microbiota"]], "jointRPCA"))

# Compute and Store Feature Loadings

#compute dataset-specific feature loadings
dataset_specific_loadings <- .dataset_specific_loadings(rclr.tables, n.components = 3, max.iterations = 5)

#store dataset-specific feature loadings per experiment
for (i in seq_along(dataset_specific_loadings)) {
  experiment_name <- names(HintikkaXOData)[i]
  metadata(HintikkaXOData[[experiment_name]])$localRPCA_feature_loadings <- dataset_specific_loadings[[i]]
}

#view dataset-specific feature loadings per experiment
for (name in names(HintikkaXOData)) {
  cat("\nFeature Loadings for:", name, "\n")
  print(head(metadata(HintikkaXOData[[name]])$localRPCA_feature_loadings))
}

#store joint feature loadings
metadata(HintikkaXOData[["microbiota"]])$jointRPCA_feature_loadings <- result$ord.res$features

#view joint feature loadings
head(metadata(HintikkaXOData[["microbiota"]])$jointRPCA_feature_loadings)

# Benchmarking

#choose label column explicitly
target_label <- "Fat"

#common samples across all rCLR tables
common_samples <- Reduce(intersect, lapply(result$rclr.tables, colnames))

#helpers
keep_finite_cols <- function(X) {
  ok <- apply(X, 2, function(v) all(is.finite(v)))
  if (!any(ok)) stop("All columns removed by finite filter.")
  X[, ok, drop = FALSE]
}
drop_constant_cols <- function(X) {
  sds <- apply(X, 2, function(v) sd(v, na.rm = TRUE))
  keep <- is.finite(sds) & (sds > 0)
  if (!any(keep)) stop("No non-constant columns remain after filtering.")
  X[, keep, drop = FALSE]
}
prep_train_test <- function(X_train, X_test) {
  m <- colMeans(X_train, na.rm = TRUE)
  s <- apply(X_train, 2, sd, na.rm = TRUE)
  s[s == 0 | !is.finite(s)] <- 1
  list(
    Xtr = sweep(sweep(X_train, 2, m, "-"), 2, s, "/"),
    Xte = sweep(sweep(X_test,  2, m, "-"), 2, s, "/")
  )
}

#build feature sets
#concatenated rCLR -> PCA (global PCA baseline)
X_list <- lapply(result$rclr.tables, function(tbl) t(tbl[, common_samples, drop = FALSE]))
features_rclr_concat <- do.call(cbind, X_list)
features_rclr_concat <- keep_finite_cols(features_rclr_concat)
features_rclr_concat <- drop_constant_cols(features_rclr_concat)
pca_concat <- prcomp(features_rclr_concat, center = TRUE, scale. = TRUE)
features_concat_pca <- pca_concat$x[, 1:min(10, ncol(pca_concat$x)), drop = FALSE]

#per-layer PCA -> concatenate
K_pcs <- 3
dataset_specific_pca_scores <- lapply(result$rclr.tables, function(tbl) {
  X <- t(tbl[, common_samples, drop = FALSE])            
  X <- keep_finite_cols(X); X <- drop_constant_cols(X)
  pca <- prcomp(X, center = TRUE, scale. = TRUE)
  k <- min(K_pcs, ncol(pca$x))
  pca$x[, seq_len(k), drop = FALSE]
})
features_pca_concat <- do.call(cbind, dataset_specific_pca_scores)

#per-layer RPCA -> concatenate
dataset_specific_scores_aligned <- lapply(dataset_specific_scores, function(S) {
  S <- S[common_samples, , drop = FALSE]
  S <- keep_finite_cols(S); drop_constant_cols(S)
})
features_rpca_concat <- do.call(cbind, dataset_specific_scores_aligned)

#joint-RPCA shared scores
features_jointRPCA <- result$ord.res$samples[common_samples, , drop = FALSE]
features_jointRPCA <- keep_finite_cols(features_jointRPCA)
features_jointRPCA <- drop_constant_cols(features_jointRPCA)

#MOFA+ factors
res_mofa <- NULL
mofa_in <- lapply(result$rclr.tables, function(M) {
  V <- M[, common_samples, drop = FALSE]        
  ok_finite <- apply(V, 1, function(v) all(is.finite(v)))
  V <- V[ok_finite, , drop = FALSE]
  sds <- apply(V, 1, sd)
  V <- V[sds > 0 & is.finite(sds), , drop = FALSE]
  V
})

mofa <- create_mofa(mofa_in)
data_opts     <- get_default_data_options(mofa)
model_opts    <- get_default_model_options(mofa)
training_opts <- get_default_training_options(mofa)

has_basilisk_prepare <- "use_basilisk" %in% names(formals(MOFA2::prepare_mofa))
has_basilisk_run     <- "use_basilisk" %in% names(formals(MOFA2::run_mofa))

if (has_basilisk_prepare) {
  mofa <- prepare_mofa(
    object = mofa,
    data_options = data_opts,
    model_options = model_opts,
    training_options = training_opts,
    use_basilisk = TRUE
  )
} else {
  mofa <- prepare_mofa(
    object = mofa,
    data_options = data_opts,
    model_options = model_opts,
    training_options = training_opts
  )
}

set.seed(1)
if (has_basilisk_run) {
  mofa <- run_mofa(mofa, use_basilisk = TRUE)
} else {
  mofa <- run_mofa(mofa)
}

fac_list <- get_factors(mofa, factors = "all", as.data.frame = FALSE)
mofa_factors <- fac_list[[1]]  
mofa_factors <- mofa_factors[rownames(features_jointRPCA), , drop = FALSE]

ve <- MOFA2::calculate_variance_explained(mofa)
ve_global <- ve$r2_total[[1]]                   

top3 <- order(ve_global, decreasing = TRUE)[seq_len(min(3, ncol(mofa_factors)))]
mofa_top3 <- mofa_factors[, top3, drop = FALSE]

#random baseline
set.seed(1)
features_random <- matrix(rnorm(length(common_samples) * 10),
                          nrow = length(common_samples), ncol = 10,
                          dimnames = list(common_samples, paste0("rand_", 1:10)))

#labels: use target_label ("Fat")
cd <- as.data.frame(colData(HintikkaXOData)[common_samples, , drop = FALSE])
labels0 <- as.factor(cd[[target_label]])

#drop NAs and classes with <2 samples
keep_idx <- !is.na(labels0)
tab0 <- table(labels0[keep_idx])
keep_classes <- names(tab0)[tab0 >= 2]
keep_idx <- keep_idx & labels0 %in% keep_classes
if (!any(keep_idx)) stop("After filtering NAs and rare classes, no samples remain for label '", target_label, "'.")

labels <- droplevels(labels0[keep_idx])

#subset features to kept samples
features_jointRPCA <- features_jointRPCA[keep_idx, , drop = FALSE]
features_rpca_concat <- features_rpca_concat[keep_idx, , drop = FALSE]
features_pca_concat  <- features_pca_concat[keep_idx, , drop = FALSE]
features_concat_pca  <- features_concat_pca[keep_idx, , drop = FALSE]
features_random      <- features_random[keep_idx, , drop = FALSE]

#safe K for CV
tab <- table(labels)
safe_k <- max(2L, min(5L, as.integer(min(tab)), length(labels) - 1L))

detach("package:MOFA2", unload = TRUE)

evaluate_model_cv <- function(features, labels, folds = 5, ntree = 500, seed = 42) {
  set.seed(seed)
  tab <- table(labels)
  if (length(labels) < 2L || length(tab) < 2L) stop("Need >=2 samples and >=2 classes.")
  folds <- max(2L, min(as.integer(folds), as.integer(min(tab)), length(labels) - 1L))
  folds_idx <- caret::createFolds(labels, k = folds, list = TRUE, returnTrain = FALSE)
  
  accs <- numeric(length(folds_idx)); aucs <- numeric(length(folds_idx))
  for (i in seq_along(folds_idx)) {
      test_idx  <- folds_idx[[i]]
      train_idx <- setdiff(seq_along(labels), test_idx)
      Xtr <- features[train_idx, , drop = FALSE]; Xte <- features[test_idx, , drop = FALSE]
      ytr <- labels[train_idx]; yte <- labels[test_idx]
      if (length(unique(ytr)) < 2L) { accs[i] <- NA_real_; aucs[i] <- NA_real_; next }
    
      pp <- prep_train_test(Xtr, Xte)
      rf <- randomForest(x = pp$Xtr, y = ytr, ntree = ntree)
    
      yhat <- predict(rf, pp$Xte, type = "response")
      accs[i] <- mean(yhat == yte)
    
      probs <- predict(rf, pp$Xte, type = "prob")
      all_lvls <- levels(labels)
      miss <- setdiff(all_lvls, colnames(probs))
      if (length(miss)) for (mm in miss) probs <- cbind(probs, setNames(rep(0, nrow(probs)), mm))
      probs <- probs[, all_lvls, drop = FALSE]
    
      if (length(unique(yte)) < 2L) {
          aucs[i] <- NA_real_
      } else {
          aucs[i] <- tryCatch(as.numeric(pROC::multiclass.roc(yte, probs)$auc), error = function(e) NA_real_)
      }
  }
  list(accuracy = mean(accs, na.rm = TRUE), auc = mean(aucs, na.rm = TRUE))

}

#run all baselines
res_joint   <- evaluate_model_cv(features_jointRPCA, labels, folds = safe_k)
res_rpca    <- evaluate_model_cv(features_rpca_concat, labels, folds = safe_k)
res_pca     <- evaluate_model_cv(features_pca_concat,  labels, folds = safe_k)
res_concat  <- evaluate_model_cv(features_concat_pca,  labels, folds = safe_k)
res_rclr_rf <- evaluate_model_cv(features_rclr_concat[keep_idx, , drop = FALSE],
                                 labels, folds = safe_k)
res_mofa <- evaluate_model_cv(mofa_top3, labels, folds = safe_k)
res_random  <- evaluate_model_cv(features_random,      labels, folds = safe_k)

results_df <- tibble::tibble(
  Method   = c("Joint-RPCA (shared scores)",
               "Per‑layer RPCA → concat",
               "Per‑layer PCA → concat",
               "Concatenated rCLR → PCA",
               "Raw rCLR → concat",
               "MOFA+ factors",
               "Random"),
  Accuracy = c(res_joint$accuracy, res_rpca$accuracy, res_pca$accuracy, res_concat$accuracy, res_rclr_rf$accuracy, res_mofa$accuracy, res_random$accuracy),
  MacroAUC = c(res_joint$auc,      res_rpca$auc,      res_pca$auc,      res_concat$auc,     res_rclr_rf$auc,      res_mofa$auc,      res_random$auc)
) %>% arrange(desc(MacroAUC))

print(results_df)

#visualize the results
results_long <- results_df %>% tidyr::pivot_longer(c(Accuracy, MacroAUC), names_to = "Metric", values_to = "Score")
ggplot(results_long, aes(x = reorder(Method, Score), y = Score)) +
  geom_col() + coord_flip() + facet_wrap(~ Metric, scales = "free_x") +
  labs(x = NULL, y = "Score", title = paste0("Classification benchmarks (", target_label, ", ", safe_k, "-fold CV)")) +
  theme_minimal(base_size = 12)


#report dims and runtime
rep_dim <- function(X) ncol(X)
dims <- tibble::tibble(
  Method   = c("Joint-RPCA", "Per-layer RPCA", "Per-layer PCA", "Concat rCLR → PCA", "Raw rCLR", "MOFA+", "Random"),
  Dim      = c(rep_dim(features_jointRPCA), rep_dim(features_rpca_concat),
               rep_dim(features_pca_concat), rep_dim(features_concat_pca),
               rep_dim(features_rclr_concat), rep_dim(mofa_top3), rep_dim(features_random))
)

timeit <- function(expr) { t0 <- proc.time(); force(expr); as.numeric((proc.time()-t0)["elapsed"]) }
times <- tibble::tibble(
  Method = dims$Method,
  Sec    = c(
    timeit(evaluate_model_cv(features_jointRPCA, labels, folds = safe_k)),
    timeit(evaluate_model_cv(features_rpca_concat, labels, folds = safe_k)),
    timeit(evaluate_model_cv(features_pca_concat,  labels, folds = safe_k)),
    timeit(evaluate_model_cv(features_concat_pca,  labels, folds = safe_k)),
    timeit(evaluate_model_cv(features_rclr_concat[rownames(features_jointRPCA), , drop = FALSE], labels, folds = safe_k)),
    timeit(evaluate_model_cv(mofa_top3, labels, folds = safe_k)),
    timeit(evaluate_model_cv(features_random, labels, folds = safe_k))
  )
)

print(dims); print(times)

#per‑fold results + CIs + paired tests

get_fold_metrics <- function(X) {
  set.seed(42)
  idx <- caret::createFolds(labels, k = safe_k, list = TRUE, returnTrain = FALSE)
  acc <- auc <- numeric(length(idx))
  for (i in seq_along(idx)) {
    te <- idx[[i]]; tr <- setdiff(seq_along(labels), te)
    if (length(unique(labels[tr])) < 2L) {acc[i] <- NA; auc[i] <- NA; next}
    pp <- prep_train_test(X[tr, , drop = FALSE], X[te, , drop = FALSE])
    rf <- randomForest(pp$Xtr, labels[tr], ntree = 500)
    yhat <- predict(rf, pp$Xte)
    acc[i] <- mean(yhat == labels[te])
    probs <- predict(rf, pp$Xte, type = "prob")
    miss <- setdiff(levels(labels), colnames(probs))
    if (length(miss)) for (mm in miss) probs <- cbind(probs, setNames(rep(0, nrow(probs)), mm))
    probs <- probs[ , levels(labels), drop = FALSE]
    auc[i] <- tryCatch(as.numeric(pROC::multiclass.roc(labels[te], probs)$auc), error = function(e) NA)
  }
  tibble::tibble(Fold = seq_along(idx), Accuracy = acc, MacroAUC = auc)
}
fold_tbl <- dplyr::bind_rows(
  get_fold_metrics(features_jointRPCA) |> dplyr::mutate(Method = "Joint-RPCA"),
  get_fold_metrics(features_rpca_concat)|> dplyr::mutate(Method = "Per-layer RPCA"),
  get_fold_metrics(features_pca_concat) |> dplyr::mutate(Method = "Per-layer PCA"),
  get_fold_metrics(features_concat_pca) |> dplyr::mutate(Method = "Concat rCLR → PCA"),
  get_fold_metrics(features_rclr_concat[rownames(features_jointRPCA), , drop = FALSE]) |> dplyr::mutate(Method = "Raw rCLR"),
  get_fold_metrics(mofa_top3)           |> dplyr::mutate(Method = "MOFA+")
)

ci95 <- function(x){ x <- x[is.finite(x)]; m <- mean(x); s <- sd(x); n <- length(x); if(n <= 1||!is.finite(s)||s == 0) c(m,m,m) else c(m, m-1.96*s/sqrt(n), m+1.96*s/sqrt(n)) }
summary_ci <- fold_tbl |> group_by(Method) |>
  summarize(Accuracy_mean = ci95(Accuracy)[1], Accuracy_lwr = ci95(Accuracy)[2], Accuracy_upr = ci95(Accuracy)[3],
            MacroAUC_mean = ci95(MacroAUC)[1], MacroAUC_lwr = ci95(MacroAUC)[2], MacroAUC_upr = ci95(MacroAUC)[3], .groups = "drop")

#plots: fold-by-fold + CI bars
ggplot(fold_tbl, aes(x = factor(Fold), y = MacroAUC, color = Method, group = Method)) +
  geom_point(position = position_jitter(width = .1, height = 0)) + geom_line(alpha = .3) +
  labs(x = "Fold", y = "MacroAUC", title = paste0("Fold-by-fold performance (", target_label,")")) + theme_minimal(12)

ggplot(summary_ci, aes(x = reorder(Method, MacroAUC_mean), y = MacroAUC_mean)) +
  geom_col() + geom_errorbar(aes(ymin = MacroAUC_lwr, ymax = MacroAUC_upr), width = .2) +
  coord_flip() + labs(x = NULL, y = "MacroAUC (mean ± 95% CI)", title = "Method comparison with 95% CIs") +
  theme_minimal(12)

# Export inputs from R to compare with the Python implementation

rclr_list <- lapply(result$rclr.tables, function(M) M[, common_samples, drop = FALSE])

#ensure deterministic order of views
view_names <- names(rclr_list)
if (is.null(view_names)) view_names <- paste0("view_", seq_along(rclr_list))

#save to disk for Python
dir.create("examples/interop", showWarnings = FALSE, recursive = TRUE)

for (i in seq_along(result$rclr.tables)) {
  fn <- file.path("examples/interop", sprintf("view_%d_rclr.csv", i))
  write.csv(result$rclr.tables[[i]][, common_samples, drop = FALSE],
            file = fn,
            row.names = TRUE,      
            fileEncoding = "UTF-8")  
}

#save ordered sample IDs and settings
write.csv(data.frame(sample = common_samples),
          file = "examples/interop/samples.csv",
          row.names = FALSE, fileEncoding = "UTF-8")

settings <- list(n_components = ncol(result$ord.res$samples),
                 max_iter = 500, seed = 42)

jsonlite::write_json(settings, "examples/interop/settings.json",
                     pretty = TRUE, auto_unbox = TRUE)

#export R joint RPCA sample scores for comparison
write.csv(result$ord.res$samples[common_samples, , drop = FALSE],
          file = "examples/interop/R_samplescores.csv",
          row.names = TRUE, fileEncoding = "UTF-8")





from pathlib import Path
import json
import os
import numpy as np
import pandas as pd
from gemelli.rpca import joint_rpca

#read settings and data
SCRIPT_DIR = Path(__file__).parent           
p = SCRIPT_DIR / "interop"

with open(p / "settings.json", encoding = "utf-8") as fh:
    settings = json.load(fh)
n_components = int(settings["n_components"])
max_iter     = int(settings["max_iter"])
seed         = int(settings["seed"])

#read views (features x samples) and ensure identical sample order
samples = pd.read_csv(p / "samples.csv", encoding = "utf-8")["sample"].tolist()

view_files = sorted([f for f in os.listdir(p) if f.endswith("_rclr.csv")])

if not view_files:
    raise FileNotFoundError(f"No view CSVs found in {p}. Expected files like 'view_1_rclr.csv'.")
    
views = []
for vf in view_files:
    V = pd.read_csv(p / vf, index_col = 0, encoding = "utf-8")  
    #ensure all requested samples exist
    missing = [s for s in samples if s not in V.columns]
    if missing:
        raise ValueError(f"{vf} is missing {len(missing)} samples found in samples.csv. "
                         f"First few missing: {missing[:5]}")
    V = V.loc[:, samples]  #reorder to match 'samples'
    views.append(V)

#Gemelli expects samples x features per view, so transpose each
X_list = [V.T.values for V in views]  #list of (n_samples x n_features_k)

#fit
np.random.seed(seed)
res = joint_rpca(
    tables = X_list,
    n_components = n_components,
    max_iterations = max_iter
)

S = pd.DataFrame(
    res["sample_loading"],
    index = samples,
    columns = [f"comp{i+1}" for i in range(n_components)]
)

#save outputs for comparison
S.to_csv(p / "gemelli_samplescores.csv", encoding = "utf-8")

for k, V in enumerate(views, start = 1):
    Fk = pd.DataFrame(
        res["feature_loading"][k-1],
        index = V.index,
        columns = [f"comp{i+1}" for i in range(n_components)]
    )
    Fk.to_csv(p / f"gemelli_loadings_view{k}.csv", encoding = "utf-8")

print(f"[OK] Wrote {p/'gemelli_samplescores.csv'} and per-view loadings.")


```
